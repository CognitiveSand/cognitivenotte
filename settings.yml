# Conot Configuration
# Copy this file to customize settings

audio:
  # Sample rate in Hz (44100 or 48000 recommended)
  sample_rate: 48000
  # Number of audio channels (1 = mono, 2 = stereo)
  channels: 1
  # Preferred device ID (null = auto-select)
  device_id: null

recording:
  # Output directory for recordings
  output_dir: "./recordings"
  # Filename format (supports strftime placeholders)
  filename_format: "recording_%Y%m%d_%H%M%S.wav"

debug:
  # Update interval for meters in seconds
  meter_update_interval: 0.05
  # Reference level for dB calculations
  reference_db: -60.0

stt:
  # STT provider: auto | faster-whisper | whisper-cpp | qwen-asr
  # When "auto", selects most accurate provider for detected hardware
  provider: auto
  # Compute device: auto | cuda | cpu
  device: auto
  # Primary language: auto | fr | en | ...
  language: auto
  # Enable speaker diarization
  diarization: true
  # Model size: auto | large-v3 | medium | small | tiny
  # For qwen-asr: auto | 1.7B | 0.6B
  model_size: auto
  # Compute precision: auto | float16 | bfloat16 | int8 | float32
  compute_type: auto
  # Voice activity detection threshold (0.0 - 1.0)
  vad_threshold: 0.5
  # Minimum speech duration in milliseconds
  min_speech_duration_ms: 250
  # Maximum speech segment duration in seconds
  max_speech_duration_s: 30.0
  # HuggingFace token for pyannote diarization (or set HF_TOKEN env var)
  huggingface_token: null

  # Qwen3-ASR specific settings
  qwen:
    # Use vLLM backend for faster inference (requires vllm installed)
    use_vllm: false
    # Enable word-level timestamps via ForcedAligner (batch mode only)
    use_forced_aligner: false
    # GPU memory utilization for vLLM (0.0-1.0)
    gpu_memory_utilization: 0.7
